import httpx
import re
from typing import List, Dict, Any, Optional

async def generate_notebook(hf_model: str, share_id: Optional[str] = None) -> Dict[str, Any]:
    try:
        # Fetch model metadata
        async with httpx.AsyncClient() as client:
            model_response = await client.get(f"https://huggingface.co/api/models/{hf_model}")
            model_data = None
            if model_response.status_code == 200:
                model_data = model_response.json()

            # Fetch README
            readme_content = ''
            code_snippet = ''
            if model_data and model_data.get('sha'):
                readme_response = await client.get(f"https://huggingface.co/{hf_model}/raw/{model_data['sha']}/README.md")
                if readme_response.status_code == 200:
                    readme_content = readme_response.text
                    # Extract first Python code block
                    python_code_match = re.search(r"```python\n([\s\S]*?)\n```", readme_content)
                    if python_code_match:
                        code_snippet = python_code_match.group(1).strip()
                    else:
                        # Fallback to any code block
                        any_code_match = re.search(r"```\n([\s\S]*?)\n```", readme_content)
                        if any_code_match:
                            code_snippet = any_code_match.group(1).strip()

        # Generate generic snippet if no code found
        if not code_snippet:
            pipeline_tag = model_data.get('pipeline_tag', 'text-generation') if model_data else 'text-generation'
            code_snippet = generate_generic_snippet(hf_model, pipeline_tag)

        # Build notebook cells
        cells = [
            # Title cell
            {
                'cell_type': 'markdown',
                'source': [
                    f"# Alacard | {hf_model} Quickstart\n\n",
                    f"**Model:** [{hf_model}](https://huggingface.co/{hf_model})\n\n",
                    f"**License:** {model_data['license']}\n\n" if model_data and model_data.get('license') else '',
                    "*Generated by Alacard Model Arena*\n"
                ]
            },
            # Environment setup
            {
                'cell_type': 'code',
                'metadata': {},
                'source': [
                    '# Install required packages\n',
                    '!pip install transformers huggingface_hub requests --quiet\n',
                    '\n',
                    'import transformers\n',
                    'from huggingface_hub import HfApi\n',
                    'import requests\n',
                    'import json\n',
                    '\n',
                    'print("✅ Environment setup complete")\n'
                ]
            },
            # Hello cell
            {
                'cell_type': 'code',
                'metadata': {},
                'source': generate_hello_cell(hf_model, model_data.get('pipeline_tag') if model_data else None)
            },
            # Sample from model card
            {
                'cell_type': 'markdown',
                'source': [
                    '## Samples from Model Card\n\n',
                    "The following code example is extracted from the model's README:\n"
                ]
            },
            {
                'cell_type': 'code',
                'metadata': {},
                'source': [
                    '# Code from model README\n',
                    code_snippet,
                    '\n',
                    'print("\n✅ Sample code executed successfully")\n'
                ]
            }
        ]

        # Add recipe metadata if shareId provided
        if share_id:
            cells.append({
                'cell_type': 'markdown',
                'source': [
                    '## Recipe Used\n\n',
                    f"This notebook was generated from an Alacard Arena comparison.\n",
                    f"**Share ID:** [{share_id}](/share/{share_id})\n",
                    '\n',
                    '*View the complete comparison results and remix this recipe. *\n'
                ]
            })

        # Add next steps
        cells.append({
            'cell_type': 'markdown',
            'source': [
                '## Next Steps\n\n',
                '- Experiment with different prompts and parameters\n',
                '- Fine-tune the model for your specific use case\n',
                '- Share your results with the community\n',
                '- [Return to Alacard Arena](/arena) to compare more models\n'
            ]
        })

        # Build notebook structure
        notebook = {
            'cells': cells,
            'metadata': {
                'kernelspec': {
                    'display_name': 'Python 3',
                    'language': 'python',
                    'name': 'python3'
                },
                'language_info': {
                    'name': 'python',
                    'version': '3.9.0'
                }
            },
            'nbformat': 4,
            'nbformat_minor': 4
        }

        return notebook

    except Exception as error:
        print(f'Notebook generation error: {error}')
        return generate_fallback_notebook(hf_model, share_id)

def generate_generic_snippet(model_id: str, pipeline_tag: str) -> str:
    if pipeline_tag == 'text-generation':
        return f'''from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLM.from_pretrained("{model_id}")

prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)'''
    elif pipeline_tag == 'text2text-generation':
        return f'''from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForSeq2SeqLM.from_pretrained("{model_id}")

prompt = "Translate: Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)'''
    else:
        return f'''from transformers import pipeline

# Create a pipeline for the model
pipe = pipeline("{pipeline_tag}", model="{model_id}")

# Test the pipeline
result = pipe("Hello, how are you?")
print(result)'''

def generate_hello_cell(model_id: str, pipeline_tag: Optional[str] = None) -> List[str]:
    if pipeline_tag in ['text-generation', 'text2text-generation']:
        return [
            '# Hello Cell - Verify Model Access\n',
            f'from transformers import pipeline\n',
            '\n',
            f'try:\n',
            f'    pipe = pipeline("{pipeline_tag or 