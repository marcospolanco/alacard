import json
import uuid
import re
import asyncio
from typing import Dict, Any, Optional
from app.services.huggingface import HuggingFaceService
from app.models.notebook import ModelInfo

class NotebookGenerator:
    def __init__(self):
        self.hf_service = HuggingFaceService()

    async def generate_notebook(self, hf_model_id: str) -> Dict[str, Any]:
        """Generate a Jupyter notebook from a Hugging Face model"""

        # Get model information
        model_info = await self.hf_service.get_model_info(hf_model_id)
        if not model_info:
            raise ValueError(f"Model {hf_model_id} not found")

        # Get README content
        readme_content = await self.hf_service.get_model_readme(hf_model_id)

        # Extract code examples from README
        code_examples = self._extract_code_from_readme(readme_content) if readme_content else []

        # Generate notebook cells
        cells = [
            self._title_cell(model_info),
            self._setup_cell(),
            self._hello_cell(hf_model_id),
            self._model_info_cell(model_info),
            self._readme_example_cell(code_examples[0] if code_examples else None, hf_model_id),
            self._generic_example_cell(model_info),
            self._next_steps_cell(model_info)
        ]

        # Create notebook structure
        notebook = {
            "cells": cells,
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                },
                "language_info": {
                    "name": "python",
                    "version": "3.8.5"
                },
                "orig_nbformat": 4,
                "alacard": {
                    "generated_at": "2024-01-01T00:00:00Z",
                    "model_id": hf_model_id,
                    "version": "1.0.0"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 4
        }

        return {
            "notebook_content": notebook,
            "metadata": {
                "model_info": model_info.dict(),
                "generated_at": "2024-01-01T00:00:00Z",
                "cells_count": len(cells)
            }
        }

    def _extract_code_from_readme(self, readme_content: str) -> list:
        """Extract code blocks from README content"""
        if not readme_content:
            return []

        # Find Python code blocks
        python_blocks = re.findall(r'```python\n(.*?)\n```', readme_content, re.DOTALL)

        # Also find code blocks without language specified
        generic_blocks = re.findall(r'```\n(.*?)\n```', readme_content, re.DOTALL)

        # Filter for blocks that look like Python code
        all_blocks = python_blocks + [
            block for block in generic_blocks
            if any(keyword in block for keyword in ['import', 'from', 'pipeline', 'AutoModel'])
        ]

        return all_blocks[:3]  # Return first 3 code blocks

    def _title_cell(self, model_info: ModelInfo) -> Dict[str, Any]:
        """Create title and attribution cell"""
        return {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                f"# {model_info.name}\n\n",
                f"**Model ID:** `{model_info.modelId}`\n\n",
                f"**Pipeline:** {model_info.pipeline_tag or 'N/A'}\n\n",
                f"**Description:** {model_info.description or 'No description available'}\n\n",
                f"[View on Hugging Face](https://huggingface.co/{model_info.modelId})\n\n",
                "---\n\n",
                "*This notebook was automatically generated by [Alacard](https://alacard.app).*"
            ]
        }

    def _setup_cell(self) -> Dict[str, Any]:
        """Create environment setup cell"""
        return {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install transformers huggingface_hub torch\n",
                "!pip install -U sentencepiece protobuf  # Often needed for Llama models"
            ]
        }

    def _hello_cell(self, model_id: str) -> Dict[str, Any]:
        """Create basic model verification cell"""
        return {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "import torch\n",
                "\n",
                f"# Load {model_id}\n",
                "try:\n",
                f"    pipe = pipeline('text-generation', model='{model_id}', device=0 if torch.cuda.is_available() else -1)\n",
                "    print('✅ Model loaded successfully!')\n",
                "    print(f'Model device: {\"GPU\" if torch.cuda.is_available() else \"CPU\"}')\n",
                "except Exception as e:\n",
                "    print(f'❌ Error loading model: {e}')\n",
                "    print('You may need to authenticate with Hugging Face or use a different model.')"
            ]
        }

    def _model_info_cell(self, model_info: ModelInfo) -> Dict[str, Any]:
        """Create model information cell"""
        return {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": [
                "## Model Information\n\n",
                f"- **Name:** {model_info.name}\n",
                f"- **Pipeline:** {model_info.pipeline_tag or 'N/A'}\n",
                f"- **Downloads:** {model_info.downloads:,}\n",
                f"- **Likes:** {model_info.likes:,}\n",
                f"- **Tags:** {', '.join(model_info.tags)}\n\n",
                "## Usage Tips\n\n",
                "- Make sure you have sufficient GPU memory for larger models\n",
                "- Some models may require Hugging Face authentication\n",
                "- Adjust generation parameters as needed for your use case"
            ]
        }

    def _readme_example_cell(self, code_example: Optional[str], model_id: str) -> Dict[str, Any]:
        """Create README example cell"""
        if code_example:
            return {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Example from model README\n",
                    code_example
                ]
            }
        else:
            # Get model info for generic example
            model_info = asyncio.run(self.hf_service.get_model_info(model_id))
            return self._generic_example_cell(model_info)

    def _generic_example_cell(self, model_info: Optional[ModelInfo]) -> Dict[str, Any]:
        """Create generic example cell"""
        if model_info and model_info.pipeline_tag == "text-generation":
            return {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Text Generation Example\n",
                    "from transformers import pipeline\n",
                    "\n",
                    f"pipe = pipeline('text-generation', model='{model_info.modelId}')\n",
                    "\n",
                    "prompt = \"Once upon a time, in a land far, far away,\"\n",
                    "\n",
                    "# Generate text\n",
                    "output = pipe(\n",
                    "    prompt,\n",
                    "    max_length=100,\n",
                    "    num_return_sequences=1,\n",
                    "    temperature=0.7,\n",
                    "    do_sample=True\n",
                    ")\n",
                    "\n",
                    "print(output[0]['generated_text'])"
                ]
            }
        elif model_info and model_info.pipeline_tag == "text-classification":
            return {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Text Classification Example\n",
                    "from transformers import pipeline\n",
                    "\n",
                    f"pipe = pipeline('text-classification', model='{model_info.modelId}')\n",
                    "\n",
                    "texts = [\n",
                    "    \"I love this movie! It's fantastic.\",\n",
                    "    \"This was terrible and boring.\",\n",
                    "    \"The weather is okay today.\"\n",
                    "]\n",
                    "\n",
                    "results = pipe(texts)\n",
                    "\n",
                    "for text, result in zip(texts, results):\n",
                    "    print(f'Text: {text}')\n",
                    "    print(f'Label: {result[\"label\"]}, Score: {result[\"score\"]:.4f}\\n')"
                ]
            }
        else:
            return {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Generic Example\n",
                    "from transformers import AutoTokenizer, AutoModel\n",
                    "\n",
                    f"model_name = '{model_info.modelId if model_info else 'distilbert-base-uncased'}'\n",
                    "\n",
                    "# Load tokenizer and model\n",
                    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                    "model = AutoModel.from_pretrained(model_name)\n",
                    "\n",
                    "# Process some text\n",
                    "text = \"Hello, this is a test example!\"\n",
                    "inputs = tokenizer(text, return_tensors='pt')\n",
                    "outputs = model(**inputs)\n",
                    "\n",
                    "print(f'Input shape: {inputs[\"input_ids\"].shape}')\n",
                    "print(f'Output shape: {outputs.last_hidden_state.shape}')\n",
                    "print('✅ Model inference successful!')"
                ]
            }

    def _next_steps_cell(self, model_info: Optional[ModelInfo]) -> Dict[str, Any]:
        """Create next steps cell"""
        return {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n\n",
                "### Resources\n",
                f"- [Model Card](https://huggingface.co/{model_info.model_id if model_info else 'models'})\n",
                "- [Hugging Face Documentation](https://huggingface.co/docs)\n",
                "- [Transformers Documentation](https://huggingface.co/docs/transformers)\n\n",
                "### Ideas to Try\n",
                "- Fine-tune the model on your own data\n",
                "- Experiment with different generation parameters\n",
                "- Combine multiple models for complex tasks\n",
                "- Deploy the model as an API endpoint\n\n",
                "---\n\n",
                "*Happy coding! 🚀*"
            ]
        }